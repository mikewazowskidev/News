name: Google News Scraper

on:
  schedule:
    # Runs every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  update-readme:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'

      - name: Install dependencies
        run: npm install axios cheerio

      - name: Create and run script
        run: |
          cat > scrape-news.js << 'ENDOFFILE'
          const axios = require('axios');
          const cheerio = require('cheerio');
          const fs = require('fs');

          const keywords = ['AI', 'Machine Learning', 'Data Science'];
          const maxArticlesPerKeyword = 5;

          async function fetchNewsForKeyword(keyword) {
            try {
              const encodedKeyword = encodeURIComponent(keyword);
              const url = `https://news.google.com/rss/search?q=${encodedKeyword}&hl=en-US&gl=US&ceid=US:en`;
              
              const response = await axios.get(url);
              const $ = cheerio.load(response.data, { xmlMode: true });
              
              const articles = [];
              $('item').each((i, item) => {
                if (i < maxArticlesPerKeyword) {
                  const title = $(item).find('title').text();
                  const link = $(item).find('link').text();
                  const source = $(item).find('source').text();
                  
                  articles.push({
                    title,
                    link,
                    source
                  });
                }
              });
              
              return articles;
            } catch (error) {
              console.error(`Error fetching news for ${keyword}:`, error.message);
              return [];
            }
          }

          async function updateReadme() {
            let readmeContent = `# News Tracker

This repository automatically tracks news articles from Google News on selected keywords.

## How It Works

- GitHub Action runs every 6 hours
- Scrapes Google News for specified keywords
- Removes duplicate articles
- Updates this README with the latest news

## Latest News

_Last updated: ${new Date().toUTCString()}_

`;

            for (const keyword of keywords) {
              const articles = await fetchNewsForKeyword(keyword);
              
              if (articles.length > 0) {
                readmeContent += `\n### ${keyword}\n\n`;
                
                for (const article of articles) {
                  readmeContent += `- [${article.title}](${article.link})`;
                  if (article.source) {
                    readmeContent += ` - ${article.source}`;
                  }
                  readmeContent += '\n';
                }
              }
            }

            fs.writeFileSync('README.md', readmeContent);
            console.log('README.md updated successfully');
          }

          updateReadme();
          ENDOFFILE
          
          node scrape-news.js

      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add README.md
          git diff --quiet && git diff --staged --quiet || git commit -m "Update news: $(date +'%Y-%m-%d %H:%M:%S')"
          git push
