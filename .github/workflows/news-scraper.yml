name: Google News RSS Parser

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  parse-rss:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser python-dateutil

      - name: Create Python script
        run: |
          cat > parse_rss.py << 'EOF'
          import feedparser
          import datetime
          from dateutil import parser
          import time
          
          # Topics and their RSS URLs
          topics = {
              "Artificial Intelligence": "https://news.google.com/rss/search?q=artificial+intelligence&hl=en-US&gl=US&ceid=US:en",
              "Machine Learning": "https://news.google.com/rss/search?q=machine+learning&hl=en-US&gl=US&ceid=US:en",
              "Data Science": "https://news.google.com/rss/search?q=data+science&hl=en-US&gl=US&ceid=US:en",
              "Cloud Computing": "https://news.google.com/rss/search?q=cloud+computing&hl=en-US&gl=US&ceid=US:en",
              "Blockchain": "https://news.google.com/rss/search?q=blockchain&hl=en-US&gl=US&ceid=US:en",
              "Cybersecurity": "https://news.google.com/rss/search?q=cybersecurity&hl=en-US&gl=US&ceid=US:en",
              "Quantum Computing": "https://news.google.com/rss/search?q=quantum+computing&hl=en-US&gl=US&ceid=US:en",
              "Virtual Reality": "https://news.google.com/rss/search?q=virtual+reality&hl=en-US&gl=US&ceid=US:en"
          }
          
          # Create README content
          readme_content = f"""# News Tracker
          
          Automatically fetched Google News RSS feeds, showing articles from the last 24 hours.
          
          Last updated: {datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}
          
          """
          
          # Get current time and 24 hours ago
          now = datetime.datetime.now()
          day_ago = now - datetime.timedelta(days=1)
          
          # Process each topic
          for topic, url in topics.items():
              print(f"Processing {topic}")
              
              # Add topic header
              readme_content += f"\n## {topic}\n\n"
              
              try:
                  # Parse the feed
                  feed = feedparser.parse(url)
                  
                  # Filter for entries in the last 24 hours
                  recent_entries = []
                  for entry in feed.entries:
                      try:
                          # Parse the publication date
                          pub_date = parser.parse(entry.published)
                          
                          # Check if it's within the last 24 hours
                          if pub_date > day_ago:
                              recent_entries.append(entry)
                      except:
                          # Skip entries with unparseable dates
                          continue
                  
                  # Add entries to README
                  if recent_entries:
                      for entry in recent_entries:
                          # Clean up description
                          description = entry.description
                          
                          # Remove HTML tags (simple approach)
                          while '<' in description and '>' in description:
                              start = description.find('<')
                              end = description.find('>', start)
                              if start != -1 and end != -1:
                                  description = description[:start] + description[end+1:]
                              else:
                                  break
                          
                          # Replace HTML entities
                          description = description.replace('&nbsp;', ' ')
                          description = description.replace('&quot;', '"')
                          description = description.replace('&amp;', '&')
                          description = description.replace('&lt;', '<')
                          description = description.replace('&gt;', '>')
                          
                          # Limit description length
                          if len(description) > 250:
                              description = description[:247] + "..."
                          
                          # Add entry to README
                          readme_content += f"- **[{entry.title}]({entry.link})** - {description}\n\n"
                  else:
                      readme_content += "*No articles published in the last 24 hours*\n\n"
              
              except Exception as e:
                  readme_content += f"*Error processing feed: {str(e)}*\n\n"
          
          # Add footer
          readme_content += """---
          
          This README is automatically updated every 6 hours via GitHub Actions."""
          
          # Write README
          with open('README.md', 'w', encoding='utf-8') as f:
              f.write(readme_content)
          
          print("README updated successfully!")
          EOF

      - name: Run Python script
        run: python parse_rss.py

      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add README.md
          git diff --quiet && git diff --staged --quiet || git commit -m "Update news: $(date +'%Y-%m-%d %H:%M:%S')"
          git push
